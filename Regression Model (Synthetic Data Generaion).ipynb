{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10703903,"sourceType":"datasetVersion","datasetId":6633633}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:33.248585Z","iopub.execute_input":"2025-02-14T16:15:33.248887Z","iopub.status.idle":"2025-02-14T16:15:37.423016Z","shell.execute_reply.started":"2025-02-14T16:15:33.248867Z","shell.execute_reply":"2025-02-14T16:15:37.421872Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch_tabnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:37.424338Z","iopub.execute_input":"2025-02-14T16:15:37.424601Z","iopub.status.idle":"2025-02-14T16:15:40.930427Z","shell.execute_reply.started":"2025-02-14T16:15:37.424578Z","shell.execute_reply":"2025-02-14T16:15:40.929613Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:40.932037Z","iopub.execute_input":"2025-02-14T16:15:40.932272Z","iopub.status.idle":"2025-02-14T16:15:44.581838Z","shell.execute_reply.started":"2025-02-14T16:15:40.932251Z","shell.execute_reply":"2025-02-14T16:15:44.580800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install warnings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Processing n' Visualization\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Compute\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Data\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Random\nimport os\nimport random as rand\nimport timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:44.583357Z","iopub.execute_input":"2025-02-14T16:15:44.583676Z","iopub.status.idle":"2025-02-14T16:15:57.090823Z","shell.execute_reply.started":"2025-02-14T16:15:44.583652Z","shell.execute_reply":"2025-02-14T16:15:57.090057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:20:37.513660Z","iopub.execute_input":"2025-02-14T16:20:37.513996Z","iopub.status.idle":"2025-02-14T16:20:37.633696Z","shell.execute_reply.started":"2025-02-14T16:20:37.513973Z","shell.execute_reply":"2025-02-14T16:20:37.633065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n  rand.seed(seed)\n  np.random.seed(seed)\n  torch.cuda.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\n\nseed = 123\nset_seed(123)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:57.091895Z","iopub.execute_input":"2025-02-14T16:15:57.092257Z","iopub.status.idle":"2025-02-14T16:15:57.098614Z","shell.execute_reply.started":"2025-02-14T16:15:57.092226Z","shell.execute_reply":"2025-02-14T16:15:57.097908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"--------------------\n# **Data Sample**","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/wharton-basketball-dataset/games_2022.xlsx'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:57.099296Z","iopub.execute_input":"2025-02-14T16:15:57.099554Z","iopub.status.idle":"2025-02-14T16:15:57.116331Z","shell.execute_reply.started":"2025-02-14T16:15:57.099517Z","shell.execute_reply":"2025-02-14T16:15:57.115592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_excel(data_dir)\nprint(f\"Dataset Type: {type(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:15:57.117020Z","iopub.execute_input":"2025-02-14T16:15:57.117221Z","iopub.status.idle":"2025-02-14T16:16:02.722537Z","shell.execute_reply.started":"2025-02-14T16:15:57.117193Z","shell.execute_reply":"2025-02-14T16:16:02.721676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ts = df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:02.725330Z","iopub.execute_input":"2025-02-14T16:16:02.725800Z","iopub.status.idle":"2025-02-14T16:16:02.771392Z","shell.execute_reply.started":"2025-02-14T16:16:02.725773Z","shell.execute_reply":"2025-02-14T16:16:02.770620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------------------------\n# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"df_ts = df_ts.drop(columns = ['OT_length_min_tot', 'attendance', 'tz_dif_H_E', 'opponent_team_score', \n                        'team_score', 'home_away', 'notD1_incomplete', 'largest_lead'])\ndf_ts = df_ts.dropna()\ndf_ts['home_away_NS'] = df_ts['home_away_NS'].replace({\n    1: 1, -1: 0, 0: 2\n})\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# List of columns to normalize\nstats_to_normalize = ['FGA_2', 'FGM_2', 'FGA_3', 'FGM_3', \n                      'FTA', 'FTM', 'AST', 'BLK', 'STL', 'TOV', \n                      'TOV_team', 'DREB', 'OREB', 'F_tech', 'F_personal', \n                      'rest_days', 'prev_game_dist', 'travel_dist']\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# Apply MinMaxScaler only to the selected stats\ndf_ts[stats_to_normalize] = scaler.fit_transform(df_ts[stats_to_normalize])\n\nprint(df_ts.head())  # Check normalized values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:02.772876Z","iopub.execute_input":"2025-02-14T16:16:02.773118Z","iopub.status.idle":"2025-02-14T16:16:02.804564Z","shell.execute_reply.started":"2025-02-14T16:16:02.773095Z","shell.execute_reply":"2025-02-14T16:16:02.803774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Data = {}\nX = []\ny = []\nstart_col = 'FGA_2'\nend_col = 'F_personal'\n\nstart_col_nx = 'rest_days'\nend_col_nx = 'travel_dist'\n# Loop through all Teams in Dataset\nfor i ,team in enumerate(df_ts['team'].unique()):\n    team_data = df_ts[df_ts['team'] == team]\n    for idx in range(len(team_data) - 1):\n        # Features = past game (FGA_2 to F_personal) + current game stats (rest_days to travel_dist)\n        past_game = team_data.iloc[idx]\n        next_game = team_data.iloc[idx + 1]\n        \n        past_stats = past_game.loc[start_col : end_col].values\n        next_stats = next_game.loc[start_col_nx:end_col_nx].values\n\n        # We predict next game FGA_2 to F_personal\n        label = next_game.loc[start_col:end_col].values\n\n        if idx == 0 and i == 0:\n            combined_stats_x = np.concatenate((past_stats, next_stats))\n            print(combined_stats_x)\n            print(f\"Type of combined_stats: {type(combined_stats_x)}\")\n            print(f\"First Index: {combined_stats_x[0]}\")\n            print(\"-\"*59)\n            print(f\"Label: {label}\")\n            print(f\"Type of Label: {type(label)}\")\n            print(f\"First Index: {label[0]}\")\n\n        combined_stats = np.concatenate((past_stats, next_stats))\n        X.append(combined_stats)\n        y.append(label)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:02.832644Z","iopub.execute_input":"2025-02-14T16:16:02.832919Z","iopub.status.idle":"2025-02-14T16:16:07.583208Z","shell.execute_reply.started":"2025-02-14T16:16:02.832892Z","shell.execute_reply":"2025-02-14T16:16:07.582546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"First Features: {X[0]}\")\nprint(f\"Number of Features: {len(X[0])}\")\n\nprint(\"-\"*59)\nprint(f\"First Labels: {y[0]}\")\nprint(f\"Number of Features to Predict: {len(y[0])}\")\n\nprint(f\"Type: {type(X), type(y)}\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.583981Z","iopub.execute_input":"2025-02-14T16:16:07.584190Z","iopub.status.idle":"2025-02-14T16:16:07.590386Z","shell.execute_reply.started":"2025-02-14T16:16:07.584172Z","shell.execute_reply":"2025-02-14T16:16:07.589600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.array(X, dtype=np.float32)\ny = np.array(y, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.591150Z","iopub.execute_input":"2025-02-14T16:16:07.591371Z","iopub.status.idle":"2025-02-14T16:16:07.653869Z","shell.execute_reply.started":"2025-02-14T16:16:07.591345Z","shell.execute_reply":"2025-02-14T16:16:07.653053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33, random_state=42)\n\nprint(X_train.shape)  # Should be (num_samples, num_features) → (N, 19)\nprint(y_train.shape)  # Should be (num_samples, num_outputs) → (N, 15)\nprint(f\"Type of X_train and X_test: {type(X_train)} | {type(X_test)}\")\nprint(X_train.dtype, y_train.dtype)\nprint(X_val.dtype, y_val.dtype)\nprint(X_test.dtype, y_test.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.654947Z","iopub.execute_input":"2025-02-14T16:16:07.655242Z","iopub.status.idle":"2025-02-14T16:16:07.669411Z","shell.execute_reply.started":"2025-02-14T16:16:07.655214Z","shell.execute_reply":"2025-02-14T16:16:07.668663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass bkb_dataset(Dataset):\n    def __init__(self, data, label):\n        self.data = data\n        self.label = label\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.data[idx], dtype=torch.float32)  # Use float32\n        label = torch.tensor(self.label[idx], dtype=torch.float32)  # Use float32\n\n        return {\"input_ids\": feature, \"labels\": label}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.670205Z","iopub.execute_input":"2025-02-14T16:16:07.670417Z","iopub.status.idle":"2025-02-14T16:16:07.676749Z","shell.execute_reply.started":"2025-02-14T16:16:07.670398Z","shell.execute_reply":"2025-02-14T16:16:07.676086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = bkb_dataset(\n    X_train,\n    y_train,\n)\n\nval_set = bkb_dataset(\n    X_val,\n    y_val,\n)\n\ntest_set = bkb_dataset(\n    X_test,\n    y_test,\n)\nprint(f\"Length of train_set: {len(train_set)}\")\nprint(f\"Length of test_set: {len(test_set)}\")\nprint(f\"Length of val_set: {len(val_set)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.677372Z","iopub.execute_input":"2025-02-14T16:16:07.677635Z","iopub.status.idle":"2025-02-14T16:16:07.690163Z","shell.execute_reply.started":"2025-02-14T16:16:07.677615Z","shell.execute_reply":"2025-02-14T16:16:07.689350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_batch = 256\ntest_batch = 32\n\ntrain_loader = DataLoader(\n    train_set,\n    batch_size = train_batch,\n    shuffle = True\n)\n\nval_loader = DataLoader(\n    val_set,\n    batch_size = test_batch,\n    shuffle = False\n)\n\ntest_loader = DataLoader(\n    test_set,\n    batch_size = test_batch,\n    shuffle = False\n)\nprint(f\"Length train_loader: {len(train_loader)}\")\nprint(f\"Length test_loader: {len(test_loader)}\")\nprint(f\"Length val_loader: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.691024Z","iopub.execute_input":"2025-02-14T16:16:07.691298Z","iopub.status.idle":"2025-02-14T16:16:07.703156Z","shell.execute_reply.started":"2025-02-14T16:16:07.691272Z","shell.execute_reply":"2025-02-14T16:16:07.702535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Login to Hugging Face\nlogin(HUGGINGFACE_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.703964Z","iopub.execute_input":"2025-02-14T16:16:07.704227Z","iopub.status.idle":"2025-02-14T16:16:07.960142Z","shell.execute_reply.started":"2025-02-14T16:16:07.704208Z","shell.execute_reply":"2025-02-14T16:16:07.959587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:07.960812Z","iopub.execute_input":"2025-02-14T16:16:07.961024Z","iopub.status.idle":"2025-02-14T16:16:08.031572Z","shell.execute_reply.started":"2025-02-14T16:16:07.961006Z","shell.execute_reply":"2025-02-14T16:16:08.030817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass FeatureGrouping(nn.Module):\n    def __init__(self, num_features=19, max_groups=3, embed_dim=4, output_dim=15, resnet_model=\"resnet50\", finetune=False):\n        super().__init__()\n        self.num_features = num_features\n        self.max_groups = max_groups\n        self.embed_dim = embed_dim\n        self.output_dim = output_dim\n\n        # Embedding for home_away (3 categories: Home, Away, Neutral)\n        self.home_away_embed = nn.Embedding(3, embed_dim)\n\n        # Adjust feature count after embedding replacement\n        self.adjusted_num_features = num_features - 1 + embed_dim  # 22 total features\n\n        # Calculate the maximum features per group to ensure consistent dimensions\n        self.max_features_per_group = self.adjusted_num_features\n\n        # Learnable logits for feature assignment\n        self.assignment_logits = nn.Parameter(torch.randn(self.adjusted_num_features, max_groups))\n\n        # Self-Attention layers for each possible number of groups\n        self.attention_layers = nn.ModuleDict({\n            f\"attn_{g}\": nn.MultiheadAttention(\n                embed_dim=self.max_features_per_group,\n                num_heads=1,\n                batch_first=True\n            )\n            for g in range(1, max_groups + 1)\n        })\n\n        # Reduce channels before ResNet\n        self.channel_reducer = nn.Conv2d(in_channels=max_groups, out_channels=3, kernel_size=1)\n\n        # Pretrained ResNet model\n        self.resnet = timm.create_model(resnet_model, pretrained=True)\n        in_features = self.resnet.get_classifier().in_features\n        self.resnet.reset_classifier(0)\n\n        # Final regression head\n        self.fc = nn.Linear(in_features, output_dim)\n\n        # Set ResNet layers to trainable or frozen based on finetune flag\n        if finetune:\n            for param in self.resnet.parameters():\n                param.requires_grad = True\n        else:\n            for param in self.resnet.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n\n        # Extract home_away index and convert to embeddings\n        home_away_idx = x[:, 17].long().clamp(0, 2)\n        home_away_embed = self.home_away_embed(home_away_idx)\n        x = torch.cat([x[:, :17], home_away_embed, x[:, 18:]], dim=1)\n\n        # Hard feature assignment\n        assignment_hard = torch.argmax(self.assignment_logits, dim=1)\n\n        all_group_outputs = []\n\n        # Process different group configurations\n        for num_groups in range(1, self.max_groups + 1):\n            # Split features into groups\n            groups = []\n            features_per_group = self.adjusted_num_features // num_groups\n            \n            for g in range(num_groups):\n                start_idx = g * features_per_group\n                end_idx = min(start_idx + features_per_group, self.adjusted_num_features)\n                group_features = x[:, start_idx:end_idx]\n                \n                # Pad to match max_features_per_group\n                if group_features.shape[1] < self.max_features_per_group:\n                    pad_size = self.max_features_per_group - group_features.shape[1]\n                    padding = torch.zeros(batch_size, pad_size, device=x.device)\n                    group_features = torch.cat([group_features, padding], dim=1)\n                \n                groups.append(group_features)\n\n            # Process each group with attention\n            processed_groups = []\n            for g in range(num_groups):\n                group_features = groups[g].unsqueeze(1)\n                attn_output, _ = self.attention_layers[f\"attn_{num_groups}\"](\n                    group_features, group_features, group_features)\n                processed_groups.append(attn_output)\n\n            # Combine processed groups\n            group_output = torch.cat(processed_groups, dim=1)\n            \n            # Pad to match max_groups if necessary\n            if num_groups < self.max_groups:\n                padding = torch.zeros(\n                    batch_size,\n                    self.max_groups - num_groups,\n                    self.max_features_per_group,\n                    device=x.device\n                )\n                group_output = torch.cat([group_output, padding], dim=1)\n            \n            all_group_outputs.append(group_output)\n\n        # Stack all configurations\n        x_final = torch.stack(all_group_outputs, dim=1)  # [B, max_groups, max_groups, Features]\n        \n        # Reshape for channel reducer\n        x_final = x_final.mean(dim=1)  # [B, max_groups, Features]\n        x_final = x_final.permute(0, 2, 1)  # [B, Features, max_groups]\n        x_final = x_final.mean(dim=1).unsqueeze(-1).unsqueeze(-1)  # [B, max_groups, 1, 1]\n        \n        # Apply channel reduction\n        x_final = self.channel_reducer(x_final)  # [B, 3, 1, 1]\n        \n        # Prepare for ResNet\n        x_final = x_final.expand(-1, -1, 224, 224)  # [B, 3, 224, 224]\n\n        # Process through ResNet and final layer\n        x_final = self.resnet(x_final)\n        output = self.fc(x_final)\n\n        return output\n\n# Example usage\nbatch_size = 1\nx = torch.randn(batch_size, 19)  # Example input features\nmodel = FeatureGrouping(resnet_model=\"resnet50\", finetune=False)  # Finetune ResNet\noutput = model(x)\nprint(output.shape)  # Expected: (B, 15)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2025-02-14T16:21:22.089091Z","iopub.execute_input":"2025-02-14T16:21:22.089424Z","iopub.status.idle":"2025-02-14T16:21:22.687793Z","shell.execute_reply.started":"2025-02-14T16:21:22.089396Z","shell.execute_reply":"2025-02-14T16:21:22.686915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PreTrainedModel, PretrainedConfig\n\nclass FeatureGroupingConfig(PretrainedConfig):\n    model_type = \"feature_grouping\"\n\n    def __init__(self, num_features=19, output_dim=15, **kwargs):\n        super().__init__(**kwargs)\n        self.num_features = num_features\n        self.output_dim = output_dim\n\nclass FeatureGroupingModel(PreTrainedModel):\n    config_class = FeatureGroupingConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = FeatureGrouping(\n            num_features=config.num_features, \n            output_dim=config.output_dim\n        )\n\n    def forward(self, input_ids, labels=None):\n        output = self.model(input_ids)\n\n        loss = None\n        if labels is not None:\n            loss = F.mse_loss(output, labels)  # Mean Squared Error for regression\n\n        return {\"loss\": loss, \"logits\": output} if loss is not None else {\"logits\": output}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:09.723575Z","iopub.execute_input":"2025-02-14T16:16:09.723812Z","iopub.status.idle":"2025-02-14T16:16:21.847194Z","shell.execute_reply.started":"2025-02-14T16:16:09.723783Z","shell.execute_reply":"2025-02-14T16:16:21.846478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.array(predictions)\n    labels = np.array(labels)\n\n    mse_per_feature = ((predictions - labels) ** 2).mean(axis=0)  # (15,)\n    mse_mean = mse_per_feature.mean()  # Overall MSE\n\n    print(f\"\\n📢 Epoch MSE: {mse_mean:.4f}\")  # Print MSE for each epoch\n\n    return {\"mse_mean\": mse_mean}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:17:51.488723Z","iopub.execute_input":"2025-02-14T16:17:51.489046Z","iopub.status.idle":"2025-02-14T16:17:51.494379Z","shell.execute_reply.started":"2025-02-14T16:17:51.489022Z","shell.execute_reply":"2025-02-14T16:17:51.493403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback\n\nclass LossLoggerCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            print(f\"📢 Epoch {state.epoch:.0f}: Train Loss = {logs['loss']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:17:52.951409Z","iopub.execute_input":"2025-02-14T16:17:52.951732Z","iopub.status.idle":"2025-02-14T16:17:52.956854Z","shell.execute_reply.started":"2025-02-14T16:17:52.951707Z","shell.execute_reply":"2025-02-14T16:17:52.955589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nfrom transformers import Trainer, TrainingArguments\n\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./WhartonDS_RegressionModel\",\n    learning_rate=1e-5,\n    eval_strategy=\"epoch\",  # Evaluate at each epoch\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",  # Log every epoch\n    logging_dir=\"./logs\",\n    logging_steps=1,  # Log at every step\n    per_device_train_batch_size=128,\n    per_device_eval_batch_size=32,\n    num_train_epochs=60,\n    load_best_model_at_end = True,\n    push_to_hub=True,\n    optim='adamw_torch',\n    report_to=\"none\"\n)\n\n# Initialize Model\nmodel = FeatureGroupingModel(FeatureGroupingConfig())\n\ntrainer = Trainer(\n    model=model.to(device),\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=val_set,\n    #callbacks=[LossLoggerCallback()]  # Add the callback here\n)\n\n# Train the Model\ntrainer.train()\n\n# Save Model\ntrainer.save_model(\"./WhartonDS_RegressionModel\")\n\n# Save Model to Hugging Face Hub\ntrainer.push_to_hub(\"KanWasTaken/RegressionModel_4DataGen_Wharton\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Test**","metadata":{}},{"cell_type":"code","source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val1, X_test1, y_val1, y_test1 = train_test_split(X_test1, y_test1, test_size=0.33, random_state=42)\n\nprint(X_train1.shape)  # Should be (num_samples, num_features) → (N, 19)\nprint(y_train1.shape)  # Should be (num_samples, num_outputs) → (N, 15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:28.153317Z","iopub.status.idle":"2025-02-14T16:16:28.153614Z","shell.execute_reply":"2025-02-14T16:16:28.153472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor\n\n# Wrap XGBRegressor to handle multi-output regression\nmodel = MultiOutputRegressor(XGBRegressor(\n    objective='reg:squarederror',  \n    n_estimators=100,  \n    learning_rate=0.1  \n))\n\n# Train model\nmodel.fit(X_train1, y_train1)  \n\ny_pred = model.predict(X_test1)\nprint(y_pred.shape)  # Should be (N_test, 15)\n\nfrom sklearn.metrics import mean_squared_error\n\nrmse = mean_squared_error(y_test1, y_pred, multioutput='raw_values')  # RMSE for each label\nprint(\"RMSE per output:\", rmse)\nprint(\"Average RMSE:\", rmse.mean())  # Average RMSE across all 15 labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:28.154725Z","iopub.status.idle":"2025-02-14T16:16:28.155066Z","shell.execute_reply":"2025-02-14T16:16:28.154887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_pred_home = model.predict(np.array([X_train1[0]]))  # Ensures a 2D shape (1, 19)\nX_pred_away = model.predict(np.array([X_train1[1]]))\nprint(X_pred_home)\nprint(X_pred_away)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:28.155859Z","iopub.status.idle":"2025-02-14T16:16:28.156168Z","shell.execute_reply":"2025-02-14T16:16:28.156055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetRegressor\n\nX_train1 = X_train1.astype(np.float32)\ny_train1 = y_train1.astype(np.float32)\nX_val1 = X_val1.astype(np.float32)\ny_val1 = y_val1.astype(np.float32)\n\nX_test1 = X_test1.astype(np.float32)\ny_test1 = y_test1.astype(np.float32)\n\nif y_test1.ndim == 1:\n    y_test1 = y_test1.reshape(-1, 1)\n\nif y_train1.ndim == 1:\n    y_train1 = y_train1.reshape(-1, 1)\nif y_val1.ndim == 1:\n    y_val1 = y_val1.reshape(-1, 1)\n    \nmodel = TabNetRegressor()\nmodel.fit(X_train1, y_train1, eval_set=[(X_val1, y_val1)])\npreds = model.predict(X_test1)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-02-14T16:16:28.156965Z","iopub.status.idle":"2025-02-14T16:16:28.157221Z","shell.execute_reply":"2025-02-14T16:16:28.157121Z"}},"outputs":[],"execution_count":null}]}